{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2nd MODEL START\n",
    "import logging\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from util import encoder, csv_helper, normalizer as norm\n",
    "\n",
    "N_INPUT = 50000\n",
    "N_TRAIN = int(N_INPUT / 0.9)\n",
    "N_TEST = int(N_INPUT / 0.9 * 0.1)\n",
    "N_CLASSES = 50\n",
    "LEARNING_RATE = 0.001\n",
    "N_EPOCHS = 20000\n",
    "N_STEPS = 100\n",
    "N_HIDDEN_NEURONS = [10, 12, 24, 48]\n",
    "N_HIDDEN_LAYERS = len(N_HIDDEN_NEURONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anonymous functions for adding sigmoid and softmax layer as wells as\n",
    "# for initializing variables with zeros and uniform random values between\n",
    "# -1 and +1.\n",
    "act = lambda l, w, b: tf.nn.relu(tf.add(tf.matmul(l, w), b))\n",
    "soft = lambda l, w, b: tf.nn.softmax(tf.add(tf.matmul(l, w), b))\n",
    "zeros = lambda h: tf.Variable(tf.zeros([h]))\n",
    "random = lambda i, o: tf.Variable(tf.random_uniform([i, o], -1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    # Setup neural net: weights, biases and connect layers\n",
    "    weights = [random(input_size, N_HIDDEN_NEURONS[0])]\n",
    "    biases = [zeros(N_HIDDEN_NEURONS[0])]\n",
    "    layers = [act(ph_in, weights[0], biases[0])]\n",
    "    test_layers = [act(ph_test_in, weights[0], biases[0])]\n",
    "\n",
    "    # for i in range(1, N_HIDDEN_LAYERS + 1):\n",
    "    for i in range(1, len(N_HIDDEN_NEURONS)):\n",
    "        weights.append(random(N_HIDDEN_NEURONS[i - 1], N_HIDDEN_NEURONS[i]))\n",
    "        biases.append(zeros(N_HIDDEN_NEURONS[i]))\n",
    "        layers.append(act(layers[i - 1], weights[i], biases[i]))\n",
    "        test_layers.append(act(test_layers[i - 1], weights[i], biases[i]))\n",
    "\n",
    "    weights.append(random(N_HIDDEN_NEURONS[N_HIDDEN_LAYERS - 1], N_CLASSES))\n",
    "    biases.append(zeros(N_CLASSES))\n",
    "    layers.append(soft(layers[N_HIDDEN_LAYERS - 1], weights[N_HIDDEN_LAYERS], biases[N_HIDDEN_LAYERS]))\n",
    "    test_layers.append(\n",
    "        soft(test_layers[N_HIDDEN_LAYERS - 1], weights[N_HIDDEN_LAYERS], biases[N_HIDDEN_LAYERS]))\n",
    "\n",
    "    return layers[N_HIDDEN_LAYERS], test_layers[N_HIDDEN_LAYERS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record():\n",
    "#     plt.plot(losses, 'b')\n",
    "#     plt.ylabel('Loss ')\n",
    "#     plt.draw()\n",
    "#     plt.pause(0.001)\n",
    "    logger.info('- Epoch = ' + str(epoch) + ', Loss = ' + str(actual_loss) + ', Train Coverage: ' +\n",
    "                str(acc_train) + ', Test Coverage: ' + str(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-05 19:58:24,656 INFO     - Epoch = 100, Loss = 31552.725, Train Coverage: 0.061650615, Test Coverage: 0.05580558\n",
      "2020-11-05 19:59:00,401 INFO     - Epoch = 200, Loss = 26431.195, Train Coverage: 0.14709747, Test Coverage: 0.1449145\n",
      "2020-11-05 19:59:35,847 INFO     - Epoch = 300, Loss = 21958.16, Train Coverage: 0.32123122, Test Coverage: 0.31719172\n",
      "2020-11-05 20:00:11,441 INFO     - Epoch = 400, Loss = 15619.239, Train Coverage: 0.55386555, Test Coverage: 0.5539154\n",
      "2020-11-05 20:00:46,831 INFO     - Epoch = 500, Loss = 13742.881, Train Coverage: 0.60595804, Test Coverage: 0.6034203\n",
      "2020-11-05 20:01:22,024 INFO     - Epoch = 600, Loss = 13011.883, Train Coverage: 0.6322563, Test Coverage: 0.6307831\n",
      "2020-11-05 20:01:57,191 INFO     - Epoch = 700, Loss = 12560.665, Train Coverage: 0.6494825, Test Coverage: 0.64464444\n",
      "2020-11-05 20:02:32,442 INFO     - Epoch = 800, Loss = 12233.347, Train Coverage: 0.66260463, Test Coverage: 0.6579658\n",
      "2020-11-05 20:03:07,472 INFO     - Epoch = 900, Loss = 11994.358, Train Coverage: 0.67556477, Test Coverage: 0.6691269\n",
      "2020-11-05 20:03:42,536 INFO     - Epoch = 1000, Loss = 11770.03, Train Coverage: 0.68699485, Test Coverage: 0.6793879\n",
      "2020-11-05 20:04:17,661 INFO     - Epoch = 1100, Loss = 11584.925, Train Coverage: 0.702187, Test Coverage: 0.6921692\n",
      "2020-11-05 20:04:52,811 INFO     - Epoch = 1200, Loss = 11450.696, Train Coverage: 0.7121411, Test Coverage: 0.7067507\n",
      "2020-11-05 20:05:27,644 INFO     - Epoch = 1300, Loss = 11327.446, Train Coverage: 0.7209792, Test Coverage: 0.7179118\n",
      "2020-11-05 20:06:02,528 INFO     - Epoch = 1400, Loss = 11301.449, Train Coverage: 0.71543515, Test Coverage: 0.71143115\n",
      "2020-11-05 20:06:37,591 INFO     - Epoch = 1500, Loss = 11190.768, Train Coverage: 0.73411936, Test Coverage: 0.7323132\n",
      "2020-11-05 20:07:12,612 INFO     - Epoch = 1600, Loss = 11144.551, Train Coverage: 0.7331473, Test Coverage: 0.72853285\n",
      "2020-11-05 20:07:47,687 INFO     - Epoch = 1700, Loss = 11097.972, Train Coverage: 0.7413734, Test Coverage: 0.73969394\n",
      "2020-11-05 20:08:22,674 INFO     - Epoch = 1800, Loss = 11060.81, Train Coverage: 0.74230945, Test Coverage: 0.7420342\n",
      "2020-11-05 20:08:57,627 INFO     - Epoch = 1900, Loss = 11024.531, Train Coverage: 0.7414994, Test Coverage: 0.7407741\n",
      "2020-11-05 20:09:32,750 INFO     - Epoch = 2000, Loss = 10987.658, Train Coverage: 0.73809737, Test Coverage: 0.7341134\n",
      "2020-11-05 20:10:07,740 INFO     - Epoch = 2100, Loss = 10954.002, Train Coverage: 0.7388354, Test Coverage: 0.7346535\n",
      "2020-11-05 20:10:44,266 INFO     - Epoch = 2200, Loss = 10922.277, Train Coverage: 0.7424714, Test Coverage: 0.74347436\n",
      "2020-11-05 20:11:19,290 INFO     - Epoch = 2300, Loss = 10890.475, Train Coverage: 0.7402034, Test Coverage: 0.7391539\n",
      "2020-11-05 20:11:54,166 INFO     - Epoch = 2400, Loss = 10861.848, Train Coverage: 0.7314553, Test Coverage: 0.7267327\n",
      "2020-11-05 20:12:29,105 INFO     - Epoch = 2500, Loss = 10816.303, Train Coverage: 0.7327873, Test Coverage: 0.73051304\n",
      "2020-11-05 20:13:04,303 INFO     - Epoch = 2600, Loss = 10772.51, Train Coverage: 0.7395374, Test Coverage: 0.73969394\n",
      "2020-11-05 20:13:38,560 INFO     - Epoch = 2700, Loss = 10720.378, Train Coverage: 0.7314193, Test Coverage: 0.72637266\n",
      "2020-11-05 20:14:12,619 INFO     - Epoch = 2800, Loss = 10668.93, Train Coverage: 0.7426514, Test Coverage: 0.7405941\n",
      "2020-11-05 20:14:47,032 INFO     - Epoch = 2900, Loss = 10633.021, Train Coverage: 0.7405274, Test Coverage: 0.739874\n",
      "2020-11-05 20:15:21,365 INFO     - Epoch = 3000, Loss = 10605.166, Train Coverage: 0.74349743, Test Coverage: 0.7414942\n",
      "2020-11-05 20:15:55,909 INFO     - Epoch = 3100, Loss = 10582.734, Train Coverage: 0.73739535, Test Coverage: 0.7350135\n",
      "2020-11-05 20:16:30,349 INFO     - Epoch = 3200, Loss = 10563.092, Train Coverage: 0.7425254, Test Coverage: 0.73861384\n",
      "2020-11-05 20:17:04,449 INFO     - Epoch = 3300, Loss = 10529.487, Train Coverage: 0.7412114, Test Coverage: 0.7380738\n",
      "2020-11-05 20:17:38,746 INFO     - Epoch = 3400, Loss = 10500.527, Train Coverage: 0.7427774, Test Coverage: 0.7391539\n",
      "2020-11-05 20:18:12,901 INFO     - Epoch = 3500, Loss = 10478.234, Train Coverage: 0.74441546, Test Coverage: 0.740054\n",
      "2020-11-05 20:18:47,151 INFO     - Epoch = 3600, Loss = 10455.955, Train Coverage: 0.7428314, Test Coverage: 0.73789376\n",
      "2020-11-05 20:19:21,514 INFO     - Epoch = 3700, Loss = 10446.847, Train Coverage: 0.74409145, Test Coverage: 0.7409541\n",
      "2020-11-05 20:19:56,279 INFO     - Epoch = 3800, Loss = 10409.109, Train Coverage: 0.7429754, Test Coverage: 0.73771375\n",
      "2020-11-05 20:20:30,802 INFO     - Epoch = 3900, Loss = 10376.2, Train Coverage: 0.74353343, Test Coverage: 0.739874\n",
      "2020-11-05 20:21:05,496 INFO     - Epoch = 4000, Loss = 10339.704, Train Coverage: 0.74378544, Test Coverage: 0.740414\n",
      "2020-11-05 20:21:40,065 INFO     - Epoch = 4100, Loss = 10312.069, Train Coverage: 0.74526143, Test Coverage: 0.7436544\n",
      "2020-11-05 20:22:14,632 INFO     - Epoch = 4200, Loss = 10283.256, Train Coverage: 0.74598145, Test Coverage: 0.7449145\n",
      "2020-11-05 20:22:48,893 INFO     - Epoch = 4300, Loss = 10291.021, Train Coverage: 0.74382144, Test Coverage: 0.740234\n",
      "2020-11-05 20:23:23,253 INFO     - Epoch = 4400, Loss = 10255.914, Train Coverage: 0.74535143, Test Coverage: 0.74329436\n",
      "2020-11-05 20:23:57,627 INFO     - Epoch = 4500, Loss = 10237.276, Train Coverage: 0.74603546, Test Coverage: 0.7459946\n",
      "2020-11-05 20:24:32,086 INFO     - Epoch = 4600, Loss = 10223.913, Train Coverage: 0.74632347, Test Coverage: 0.7459946\n",
      "2020-11-05 20:25:06,369 INFO     - Epoch = 4700, Loss = 10211.76, Train Coverage: 0.7465755, Test Coverage: 0.7468947\n",
      "2020-11-05 20:25:40,743 INFO     - Epoch = 4800, Loss = 10199.387, Train Coverage: 0.7471695, Test Coverage: 0.74743474\n",
      "2020-11-05 20:26:15,866 INFO     - Epoch = 4900, Loss = 10192.744, Train Coverage: 0.74612546, Test Coverage: 0.74671465\n",
      "2020-11-05 20:26:50,339 INFO     - Epoch = 5000, Loss = 10173.936, Train Coverage: 0.7470615, Test Coverage: 0.7470747\n",
      "2020-11-05 20:27:24,853 INFO     - Epoch = 5100, Loss = 10169.207, Train Coverage: 0.74812347, Test Coverage: 0.7504951\n",
      "2020-11-05 20:27:59,402 INFO     - Epoch = 5200, Loss = 10142.505, Train Coverage: 0.74805146, Test Coverage: 0.7481548\n",
      "2020-11-05 20:28:33,775 INFO     - Epoch = 5300, Loss = 10123.975, Train Coverage: 0.7490415, Test Coverage: 0.7490549\n",
      "2020-11-05 20:29:08,122 INFO     - Epoch = 5400, Loss = 10108.171, Train Coverage: 0.7486095, Test Coverage: 0.7517552\n",
      "2020-11-05 20:29:42,525 INFO     - Epoch = 5500, Loss = 10081.522, Train Coverage: 0.7498515, Test Coverage: 0.7512151\n",
      "2020-11-05 20:30:17,088 INFO     - Epoch = 5600, Loss = 10028.994, Train Coverage: 0.74805146, Test Coverage: 0.75139517\n",
      "2020-11-05 20:30:51,370 INFO     - Epoch = 5700, Loss = 9992.9, Train Coverage: 0.7474395, Test Coverage: 0.7479748\n",
      "2020-11-05 20:31:25,883 INFO     - Epoch = 5800, Loss = 9968.414, Train Coverage: 0.74769145, Test Coverage: 0.7479748\n",
      "2020-11-05 20:32:00,201 INFO     - Epoch = 5900, Loss = 9930.539, Train Coverage: 0.7467735, Test Coverage: 0.7470747\n",
      "2020-11-05 20:32:34,517 INFO     - Epoch = 6000, Loss = 9891.791, Train Coverage: 0.74617946, Test Coverage: 0.7468947\n",
      "2020-11-05 20:33:08,793 INFO     - Epoch = 6100, Loss = 9839.682, Train Coverage: 0.74542344, Test Coverage: 0.74743474\n",
      "2020-11-05 20:33:43,130 INFO     - Epoch = 6200, Loss = 9761.563, Train Coverage: 0.7470435, Test Coverage: 0.7486949\n",
      "2020-11-05 20:34:17,165 INFO     - Epoch = 6300, Loss = 9668.199, Train Coverage: 0.7482315, Test Coverage: 0.7517552\n",
      "2020-11-05 20:34:51,646 INFO     - Epoch = 6400, Loss = 9537.898, Train Coverage: 0.75427955, Test Coverage: 0.75643563\n",
      "2020-11-05 20:35:26,096 INFO     - Epoch = 6500, Loss = 9422.515, Train Coverage: 0.75557554, Test Coverage: 0.7582358\n",
      "2020-11-05 20:36:00,436 INFO     - Epoch = 6600, Loss = 9322.21, Train Coverage: 0.7627936, Test Coverage: 0.7663366\n",
      "2020-11-05 20:36:34,999 INFO     - Epoch = 6700, Loss = 9230.268, Train Coverage: 0.76223564, Test Coverage: 0.7661566\n",
      "2020-11-05 20:37:09,646 INFO     - Epoch = 6800, Loss = 9147.701, Train Coverage: 0.7618216, Test Coverage: 0.7657966\n",
      "2020-11-05 20:37:44,318 INFO     - Epoch = 6900, Loss = 10005.788, Train Coverage: 0.7329493, Test Coverage: 0.73627365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-05 20:38:18,541 INFO     - Epoch = 7000, Loss = 8981.782, Train Coverage: 0.76417965, Test Coverage: 0.7683168\n",
      "2020-11-05 20:38:52,660 INFO     - Epoch = 7100, Loss = 8914.21, Train Coverage: 0.76588964, Test Coverage: 0.7710171\n",
      "2020-11-05 20:39:26,698 INFO     - Epoch = 7200, Loss = 8853.621, Train Coverage: 0.7669157, Test Coverage: 0.77119714\n",
      "2020-11-05 20:40:00,622 INFO     - Epoch = 7300, Loss = 8795.615, Train Coverage: 0.7686257, Test Coverage: 0.77191716\n",
      "2020-11-05 20:40:34,391 INFO     - Epoch = 7400, Loss = 8748.1875, Train Coverage: 0.77487177, Test Coverage: 0.7782178\n",
      "2020-11-05 20:41:08,209 INFO     - Epoch = 7500, Loss = 8706.8955, Train Coverage: 0.7669877, Test Coverage: 0.77191716\n",
      "2020-11-05 20:41:42,185 INFO     - Epoch = 7600, Loss = 8638.247, Train Coverage: 0.7794618, Test Coverage: 0.7821782\n",
      "2020-11-05 20:42:16,162 INFO     - Epoch = 7700, Loss = 8580.094, Train Coverage: 0.7797678, Test Coverage: 0.7834383\n",
      "2020-11-05 20:42:50,152 INFO     - Epoch = 7800, Loss = 8530.715, Train Coverage: 0.7807218, Test Coverage: 0.7830783\n",
      "2020-11-05 20:43:24,190 INFO     - Epoch = 7900, Loss = 8491.018, Train Coverage: 0.7879039, Test Coverage: 0.79153913\n",
      "2020-11-05 20:43:58,181 INFO     - Epoch = 8000, Loss = 8432.517, Train Coverage: 0.7870579, Test Coverage: 0.7909991\n",
      "2020-11-05 20:44:32,183 INFO     - Epoch = 8100, Loss = 8378.018, Train Coverage: 0.7869499, Test Coverage: 0.789919\n",
      "2020-11-05 20:45:06,249 INFO     - Epoch = 8200, Loss = 8336.224, Train Coverage: 0.7894879, Test Coverage: 0.7918992\n",
      "2020-11-05 20:45:40,237 INFO     - Epoch = 8300, Loss = 8290.844, Train Coverage: 0.7900279, Test Coverage: 0.7918992\n",
      "2020-11-05 20:46:14,112 INFO     - Epoch = 8400, Loss = 8254.188, Train Coverage: 0.7922059, Test Coverage: 0.7938794\n",
      "2020-11-05 20:46:48,251 INFO     - Epoch = 8500, Loss = 8206.104, Train Coverage: 0.7914859, Test Coverage: 0.7931593\n",
      "2020-11-05 20:47:22,459 INFO     - Epoch = 8600, Loss = 8176.436, Train Coverage: 0.79422194, Test Coverage: 0.79783976\n",
      "2020-11-05 20:47:56,362 INFO     - Epoch = 8700, Loss = 8130.281, Train Coverage: 0.79449195, Test Coverage: 0.7992799\n",
      "2020-11-05 20:48:30,359 INFO     - Epoch = 8800, Loss = 8092.7275, Train Coverage: 0.7916839, Test Coverage: 0.79567957\n",
      "2020-11-05 20:49:04,320 INFO     - Epoch = 8900, Loss = 8054.5312, Train Coverage: 0.795446, Test Coverage: 0.79621965\n",
      "2020-11-05 20:49:38,245 INFO     - Epoch = 9000, Loss = 8034.1436, Train Coverage: 0.79928, Test Coverage: 0.8030603\n",
      "2020-11-05 20:50:12,108 INFO     - Epoch = 9100, Loss = 7997.3994, Train Coverage: 0.800342, Test Coverage: 0.8037804\n",
      "2020-11-05 20:50:46,079 INFO     - Epoch = 9200, Loss = 7950.285, Train Coverage: 0.79766, Test Coverage: 0.7992799\n",
      "2020-11-05 20:51:19,903 INFO     - Epoch = 9300, Loss = 7928.5933, Train Coverage: 0.7912879, Test Coverage: 0.79441947\n",
      "2020-11-05 20:51:54,028 INFO     - Epoch = 9400, Loss = 7900.7866, Train Coverage: 0.80302405, Test Coverage: 0.80648065\n",
      "2020-11-05 20:52:28,481 INFO     - Epoch = 9500, Loss = 7847.8164, Train Coverage: 0.80061203, Test Coverage: 0.8014401\n",
      "2020-11-05 20:53:02,505 INFO     - Epoch = 9600, Loss = 7814.4375, Train Coverage: 0.80075604, Test Coverage: 0.8021602\n",
      "2020-11-05 20:53:36,524 INFO     - Epoch = 9700, Loss = 7786.4443, Train Coverage: 0.80054003, Test Coverage: 0.80342036\n",
      "2020-11-05 20:54:10,754 INFO     - Epoch = 9800, Loss = 7746.14, Train Coverage: 0.803798, Test Coverage: 0.80540055\n",
      "2020-11-05 20:54:45,210 INFO     - Epoch = 9900, Loss = 7704.625, Train Coverage: 0.800936, Test Coverage: 0.80450046\n",
      "2020-11-05 20:55:19,211 INFO     - Epoch = 10000, Loss = 7693.1753, Train Coverage: 0.79499596, Test Coverage: 0.79945993\n",
      "2020-11-05 20:55:53,163 INFO     - Epoch = 10100, Loss = 7615.997, Train Coverage: 0.80459005, Test Coverage: 0.8070207\n",
      "2020-11-05 20:56:27,114 INFO     - Epoch = 10200, Loss = 7579.673, Train Coverage: 0.8053461, Test Coverage: 0.8079208\n",
      "2020-11-05 20:57:00,748 INFO     - Epoch = 10300, Loss = 7545.6724, Train Coverage: 0.80455405, Test Coverage: 0.80846083\n",
      "2020-11-05 20:57:34,965 INFO     - Epoch = 10400, Loss = 7546.2817, Train Coverage: 0.801998, Test Coverage: 0.80666065\n",
      "2020-11-05 20:58:09,109 INFO     - Epoch = 10500, Loss = 7481.5527, Train Coverage: 0.8055621, Test Coverage: 0.8091809\n",
      "2020-11-05 20:58:43,235 INFO     - Epoch = 10600, Loss = 7451.2715, Train Coverage: 0.80678606, Test Coverage: 0.8093609\n",
      "2020-11-05 20:59:17,208 INFO     - Epoch = 10700, Loss = 7445.566, Train Coverage: 0.80414003, Test Coverage: 0.8090009\n",
      "2020-11-05 20:59:51,208 INFO     - Epoch = 10800, Loss = 7398.3706, Train Coverage: 0.8054181, Test Coverage: 0.8093609\n",
      "2020-11-05 21:00:25,234 INFO     - Epoch = 10900, Loss = 7388.672, Train Coverage: 0.8075781, Test Coverage: 0.809721\n",
      "2020-11-05 21:00:59,209 INFO     - Epoch = 11000, Loss = 7345.385, Train Coverage: 0.80666006, Test Coverage: 0.809721\n",
      "2020-11-05 21:01:33,138 INFO     - Epoch = 11100, Loss = 7346.2554, Train Coverage: 0.8081901, Test Coverage: 0.8111611\n",
      "2020-11-05 21:02:07,238 INFO     - Epoch = 11200, Loss = 7305.7563, Train Coverage: 0.8074161, Test Coverage: 0.8106211\n",
      "2020-11-05 21:02:41,312 INFO     - Epoch = 11300, Loss = 7288.4097, Train Coverage: 0.8081001, Test Coverage: 0.810441\n",
      "2020-11-05 21:03:15,431 INFO     - Epoch = 11400, Loss = 7257.1416, Train Coverage: 0.8079381, Test Coverage: 0.8118812\n",
      "2020-11-05 21:03:49,437 INFO     - Epoch = 11500, Loss = 7233.699, Train Coverage: 0.8074341, Test Coverage: 0.8108011\n",
      "2020-11-05 21:04:23,514 INFO     - Epoch = 11600, Loss = 7257.444, Train Coverage: 0.8090721, Test Coverage: 0.8131413\n",
      "2020-11-05 21:04:57,540 INFO     - Epoch = 11700, Loss = 7228.868, Train Coverage: 0.8054001, Test Coverage: 0.810441\n",
      "2020-11-05 21:05:31,594 INFO     - Epoch = 11800, Loss = 7176.0693, Train Coverage: 0.8076321, Test Coverage: 0.810261\n",
      "2020-11-05 21:06:05,672 INFO     - Epoch = 11900, Loss = 7172.174, Train Coverage: 0.8092161, Test Coverage: 0.81332135\n",
      "2020-11-05 21:06:39,830 INFO     - Epoch = 12000, Loss = 7141.2754, Train Coverage: 0.8073621, Test Coverage: 0.8120612\n",
      "2020-11-05 21:07:14,041 INFO     - Epoch = 12100, Loss = 7126.0073, Train Coverage: 0.80702007, Test Coverage: 0.81242126\n",
      "2020-11-05 21:07:48,187 INFO     - Epoch = 12200, Loss = 7112.0996, Train Coverage: 0.8092521, Test Coverage: 0.81368136\n",
      "2020-11-05 21:08:22,595 INFO     - Epoch = 12300, Loss = 7116.95, Train Coverage: 0.80630004, Test Coverage: 0.8118812\n",
      "2020-11-05 21:08:56,661 INFO     - Epoch = 12400, Loss = 7085.058, Train Coverage: 0.8099721, Test Coverage: 0.81368136\n",
      "2020-11-05 21:09:30,935 INFO     - Epoch = 12500, Loss = 7063.364, Train Coverage: 0.80860406, Test Coverage: 0.8113411\n",
      "2020-11-05 21:10:05,115 INFO     - Epoch = 12600, Loss = 7074.825, Train Coverage: 0.80649805, Test Coverage: 0.81242126\n",
      "2020-11-05 21:10:40,455 INFO     - Epoch = 12700, Loss = 7042.797, Train Coverage: 0.8098281, Test Coverage: 0.81386137\n",
      "2020-11-05 21:11:14,492 INFO     - Epoch = 12800, Loss = 7040.3203, Train Coverage: 0.8072721, Test Coverage: 0.8120612\n",
      "2020-11-05 21:11:48,543 INFO     - Epoch = 12900, Loss = 7037.5273, Train Coverage: 0.8102061, Test Coverage: 0.81548154\n",
      "2020-11-05 21:12:22,743 INFO     - Epoch = 13000, Loss = 7021.505, Train Coverage: 0.8076141, Test Coverage: 0.8127813\n",
      "2020-11-05 21:12:57,056 INFO     - Epoch = 13100, Loss = 7015.5654, Train Coverage: 0.8105301, Test Coverage: 0.81548154\n",
      "2020-11-05 21:13:31,161 INFO     - Epoch = 13200, Loss = 7001.1694, Train Coverage: 0.81065613, Test Coverage: 0.81440145\n",
      "2020-11-05 21:14:05,193 INFO     - Epoch = 13300, Loss = 6990.8027, Train Coverage: 0.8106021, Test Coverage: 0.81530154\n",
      "2020-11-05 21:14:39,279 INFO     - Epoch = 13400, Loss = 6992.598, Train Coverage: 0.81089014, Test Coverage: 0.81440145\n",
      "2020-11-05 21:15:13,270 INFO     - Epoch = 13500, Loss = 6978.8994, Train Coverage: 0.81072813, Test Coverage: 0.81476146\n",
      "2020-11-05 21:15:47,410 INFO     - Epoch = 13600, Loss = 6970.953, Train Coverage: 0.8105301, Test Coverage: 0.81476146\n",
      "2020-11-05 21:16:21,481 INFO     - Epoch = 13700, Loss = 6969.0093, Train Coverage: 0.8074521, Test Coverage: 0.8120612\n",
      "2020-11-05 21:16:55,439 INFO     - Epoch = 13800, Loss = 6949.9175, Train Coverage: 0.8101341, Test Coverage: 0.81242126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-05 21:17:30,680 INFO     - Epoch = 13900, Loss = 6948.162, Train Coverage: 0.8102601, Test Coverage: 0.81422144\n",
      "2020-11-05 21:18:05,499 INFO     - Epoch = 14000, Loss = 6943.8735, Train Coverage: 0.8103681, Test Coverage: 0.81476146\n",
      "2020-11-05 21:18:39,950 INFO     - Epoch = 14100, Loss = 14085.757, Train Coverage: 0.72526324, Test Coverage: 0.7314131\n",
      "2020-11-05 21:19:14,418 INFO     - Epoch = 14200, Loss = 8822.782, Train Coverage: 0.7892179, Test Coverage: 0.7931593\n",
      "2020-11-05 21:19:48,856 INFO     - Epoch = 14300, Loss = 7639.6143, Train Coverage: 0.79463595, Test Coverage: 0.79711974\n",
      "2020-11-05 21:20:23,253 INFO     - Epoch = 14400, Loss = 7622.805, Train Coverage: 0.79517597, Test Coverage: 0.79711974\n",
      "2020-11-05 21:20:57,651 INFO     - Epoch = 14500, Loss = 7610.748, Train Coverage: 0.795374, Test Coverage: 0.79747975\n",
      "2020-11-05 21:21:32,062 INFO     - Epoch = 14600, Loss = 7603.9717, Train Coverage: 0.795482, Test Coverage: 0.79747975\n",
      "2020-11-05 21:22:06,523 INFO     - Epoch = 14700, Loss = 7597.7485, Train Coverage: 0.7957159, Test Coverage: 0.79747975\n",
      "2020-11-05 21:22:40,608 INFO     - Epoch = 14800, Loss = 7593.5103, Train Coverage: 0.795662, Test Coverage: 0.79729974\n",
      "2020-11-05 21:23:14,646 INFO     - Epoch = 14900, Loss = 7590.633, Train Coverage: 0.795644, Test Coverage: 0.79765975\n",
      "2020-11-05 21:23:48,621 INFO     - Epoch = 15000, Loss = 7586.839, Train Coverage: 0.795644, Test Coverage: 0.79747975\n",
      "2020-11-05 21:24:22,718 INFO     - Epoch = 15100, Loss = 7582.286, Train Coverage: 0.795608, Test Coverage: 0.79747975\n",
      "2020-11-05 21:24:56,745 INFO     - Epoch = 15200, Loss = 7579.852, Train Coverage: 0.79568, Test Coverage: 0.79747975\n",
      "2020-11-05 21:25:30,715 INFO     - Epoch = 15300, Loss = 7577.6465, Train Coverage: 0.79568, Test Coverage: 0.79747975\n",
      "2020-11-05 21:26:04,560 INFO     - Epoch = 15400, Loss = 7575.5566, Train Coverage: 0.795626, Test Coverage: 0.79711974\n",
      "2020-11-05 21:26:38,641 INFO     - Epoch = 15500, Loss = 7575.737, Train Coverage: 0.79443794, Test Coverage: 0.79711974\n",
      "2020-11-05 21:27:13,001 INFO     - Epoch = 15600, Loss = 7573.0586, Train Coverage: 0.79472595, Test Coverage: 0.79711974\n",
      "2020-11-05 21:27:47,073 INFO     - Epoch = 15700, Loss = 7571.7886, Train Coverage: 0.79657996, Test Coverage: 0.8\n",
      "2020-11-05 21:28:22,313 INFO     - Epoch = 15800, Loss = 7574.256, Train Coverage: 0.796994, Test Coverage: 0.8012601\n",
      "2020-11-05 21:29:30,005 INFO     - Epoch = 15900, Loss = 7565.8066, Train Coverage: 0.79441994, Test Coverage: 0.79711974\n",
      "2020-11-05 21:30:10,313 INFO     - Epoch = 16000, Loss = 7571.1733, Train Coverage: 0.7935019, Test Coverage: 0.79729974\n",
      "2020-11-05 21:30:49,548 INFO     - Epoch = 16100, Loss = 7562.782, Train Coverage: 0.79654396, Test Coverage: 0.8\n",
      "2020-11-05 21:31:26,815 INFO     - Epoch = 16200, Loss = 7558.161, Train Coverage: 0.79609394, Test Coverage: 0.80018\n",
      "2020-11-05 21:32:02,534 INFO     - Epoch = 16300, Loss = 7574.312, Train Coverage: 0.79310596, Test Coverage: 0.79729974\n",
      "2020-11-05 21:32:38,394 INFO     - Epoch = 16400, Loss = 7553.7437, Train Coverage: 0.79614794, Test Coverage: 0.79945993\n",
      "2020-11-05 21:33:15,725 INFO     - Epoch = 16500, Loss = 7572.8276, Train Coverage: 0.79591393, Test Coverage: 0.8007201\n",
      "2020-11-05 21:33:52,630 INFO     - Epoch = 16600, Loss = 7563.708, Train Coverage: 0.7933219, Test Coverage: 0.79783976\n",
      "2020-11-05 21:34:29,915 INFO     - Epoch = 16700, Loss = 7549.028, Train Coverage: 0.79636395, Test Coverage: 0.80018\n",
      "2020-11-05 21:35:07,084 INFO     - Epoch = 16800, Loss = 7562.116, Train Coverage: 0.79656196, Test Coverage: 0.8025203\n",
      "2020-11-05 21:35:42,883 INFO     - Epoch = 16900, Loss = 7555.8315, Train Coverage: 0.7935559, Test Coverage: 0.79819983\n",
      "2020-11-05 21:36:18,618 INFO     - Epoch = 17000, Loss = 7576.9976, Train Coverage: 0.79283595, Test Coverage: 0.79711974\n",
      "2020-11-05 21:36:57,071 INFO     - Epoch = 17100, Loss = 7546.855, Train Coverage: 0.79703, Test Coverage: 0.80036\n",
      "2020-11-05 21:37:32,917 INFO     - Epoch = 17200, Loss = 7544.8774, Train Coverage: 0.797174, Test Coverage: 0.79982\n",
      "2020-11-05 21:38:08,167 INFO     - Epoch = 17300, Loss = 7568.2783, Train Coverage: 0.795248, Test Coverage: 0.79982\n",
      "2020-11-05 21:38:45,114 INFO     - Epoch = 17400, Loss = 7550.6016, Train Coverage: 0.79712, Test Coverage: 0.8028803\n",
      "2020-11-05 21:39:20,648 INFO     - Epoch = 17500, Loss = 7544.245, Train Coverage: 0.7936279, Test Coverage: 0.79747975\n",
      "2020-11-05 21:39:55,383 INFO     - Epoch = 17600, Loss = 7535.364, Train Coverage: 0.79621994, Test Coverage: 0.79982\n",
      "2020-11-05 21:40:29,909 INFO     - Epoch = 17700, Loss = 7533.091, Train Coverage: 0.79607594, Test Coverage: 0.7992799\n",
      "2020-11-05 21:41:05,290 INFO     - Epoch = 17800, Loss = 7539.9014, Train Coverage: 0.79400593, Test Coverage: 0.79747975\n",
      "2020-11-05 21:41:41,263 INFO     - Epoch = 17900, Loss = 7534.9707, Train Coverage: 0.797192, Test Coverage: 0.79963994\n",
      "2020-11-05 21:42:15,982 INFO     - Epoch = 18000, Loss = 7538.4766, Train Coverage: 0.797084, Test Coverage: 0.8025203\n",
      "2020-11-05 21:42:50,188 INFO     - Epoch = 18100, Loss = 7530.782, Train Coverage: 0.79654396, Test Coverage: 0.7992799\n",
      "2020-11-05 21:43:24,524 INFO     - Epoch = 18200, Loss = 7554.8213, Train Coverage: 0.79513997, Test Coverage: 0.79982\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-238-77ff2f9175ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mtrain_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mph_in\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mph_out\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_out\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mtest_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mph_test_in\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mph_test_out\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_out\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 958\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    959\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1148\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m             \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)-8s %(message)s')\n",
    "    logger = logging.getLogger(__name__)\n",
    "    model_file = \"data/trained_models/model-\" + str(N_HIDDEN_LAYERS) + \"-\" + str(N_HIDDEN_NEURONS) + \".ckpt\"\n",
    "\n",
    "    # Read data from dump and map each label to its one hot vector\n",
    "    csv_input, csv_output = csv_helper._import('data/tagged_questions.csv', N_CLASSES)\n",
    "    for i in range(0, len(csv_output)):\n",
    "        csv_output[i] = encoder.one_hot(int(csv_output[i]), N_CLASSES)\n",
    "    input_size = len(csv_input[0])\n",
    "\n",
    "    # Split data into training and test set\n",
    "    train_in, train_out = csv_input[:N_TRAIN], csv_output[:N_TRAIN]\n",
    "    test_in, test_out = csv_input[N_TRAIN:N_TRAIN + N_TEST], csv_output[N_TRAIN:N_TRAIN + N_TEST]\n",
    "    train_in, test_in = norm.standard(train_in, test_in)\n",
    "\n",
    "    # Set TensorFlows placeholder for training input, target value and test input\n",
    "    ph_in = tf.placeholder(tf.float32, shape=[N_TRAIN, input_size])\n",
    "    ph_out = tf.placeholder(tf.float32, shape=[N_TRAIN, N_CLASSES])\n",
    "    ph_test_in = tf.placeholder(tf.float32, shape=[N_TEST, input_size])\n",
    "    ph_test_out = tf.placeholder(tf.float32, shape=[N_TEST, N_CLASSES])\n",
    "\n",
    "    net, test_net = build_model()\n",
    "\n",
    "    # Add node to graph that calculates mean squared error\n",
    "    LOSS = tf.nn.l2_loss(net - ph_out)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(net, 1), tf.argmax(ph_out, 1)), tf.float32))\n",
    "\n",
    "    # Initialize optimizer that uses gradient descent\n",
    "    optimizer = tf.train.AdamOptimizer(LEARNING_RATE).minimize(LOSS)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    losses = []\n",
    "    epoch = 0\n",
    "\n",
    "    # Run training and check depended on N_STEPS the relative number of correct\n",
    "    # classified training and test samples as well as plot the loss function in\n",
    "    # addition to its number of epochs.\n",
    "    acc_train = sess.run(accuracy, feed_dict={ph_in: train_in, ph_out: train_out})\n",
    "    while epoch < N_EPOCHS:\n",
    "        train_dict = {ph_in: train_in, ph_out: train_out}\n",
    "        test_dict = {ph_test_in: test_in, ph_test_out: test_out}\n",
    "        sess.run(optimizer, feed_dict=train_dict)\n",
    "        epoch += 1\n",
    "\n",
    "        if epoch % N_STEPS == 0:\n",
    "            acc_train = sess.run(accuracy, feed_dict=train_dict)\n",
    "            acc_test = sess.run(tf.reduce_mean(\n",
    "                tf.cast(tf.equal(tf.argmax(test_net, 1), tf.argmax(ph_test_out, 1)), tf.float32)), feed_dict=test_dict)\n",
    "            actual_loss = sess.run(LOSS, feed_dict=train_dict)\n",
    "            losses.append(actual_loss)\n",
    "            record()\n",
    "\n",
    "    saver.save(sess, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2 end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "word_vect = KeyedVectors.load_word2vec_format(\"SO_vectors_200.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "idToTagIndex = {}          #dict mapping post ID to a list of tag indices\n",
    "tagToTagIndex = {}         #dict mapping tag to tag index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def truncate(number, digits) -> float:\n",
    "    stepper = 10.0 ** digits\n",
    "    return math.trunc(stepper * number) / stepper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Id', 'OwnerUserId', 'CreationDate', 'ClosedDate', 'Score', 'Title',\n",
      "       'Body'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "with open(\"stacksample/Questions-ascii-1M.csv\") as question_input:\n",
    "    questions_data = pd.read_csv(question_input, engine='python')\n",
    "\n",
    "    print(questions_data.columns)\n",
    "    questions_data = questions_data[['Id', 'Body']]\n",
    "    questions_data.insert(len(questions_data.columns), 'Code', \"\")\n",
    "    \n",
    "    a = re.compile(r'<pre><code>([^<]*)</code></pre>')\n",
    "    b = re.compile(r'<.*?>')\n",
    "    stopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"])\n",
    "    \n",
    "    questions_data['Code'] = questions_data['Body'].apply(lambda x: ' '.join(re.findall(a, x)))\n",
    "\n",
    "    def clean(text):\n",
    "        # remove the code from the text\n",
    "        x = re.sub(a, '', text)\n",
    "        \n",
    "        # removes the parantheses from the text\n",
    "        x = re.sub(b, '', x)\n",
    "        \n",
    "        # replace multiple new lines\n",
    "        x = x.replace('\\n\\n', '\\n')\n",
    "        \n",
    "        # specific word converter\n",
    "        x = re.sub(r\"won't\", \"will not\", x)\n",
    "        x = re.sub(r\"can\\'t\", \"can not\", x)\n",
    "\n",
    "        # generalized words converter\n",
    "        x = re.sub(r\"n\\'t\", \" not\", x)\n",
    "        x = re.sub(r\"\\'re\", \" are\", x)\n",
    "        x = re.sub(r\"\\'s\", \" is\", x)\n",
    "        x = re.sub(r\"\\'d\", \" would\", x)\n",
    "        x = re.sub(r\"\\'ll\", \" will\", x)\n",
    "        x = re.sub(r\"\\'t\", \" not\", x)\n",
    "        x = re.sub(r\"\\'ve\", \" have\", x)\n",
    "        x = re.sub(r\"\\'m\", \" am\", x)\n",
    "        \n",
    "        #remove stopwords\n",
    "        x = ' '.join(e.lower() for e in x.split() if e.lower() not in stopwords)\n",
    "        \n",
    "        return x\n",
    "    questions_data['Body'] = questions_data['Body'].apply(clean)\n",
    "    \n",
    "#     questions_data['Body'] = questions_data['Body'].apply(nltk.tokenize.word_tokenize) #need to fix: don't convert\n",
    "#     # C#, C++ to C\n",
    "#     questions_data['Body'] = questions_data['Body'].apply(lambda x: [word for word in x if word.isalnum()])\n",
    "#     questions_data['Body'] = questions_data['Body'].apply(lambda x: [word.lower() for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "152759it [00:13, 11181.25it/s]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "idToTagIndex = {}          #dict mapping post ID to a list of tag indices\n",
    "tagToTagIndex = {}         #dict mapping tag to tag index\n",
    "tagIndexToTag = {}\n",
    "tagToFrequency = defaultdict(lambda: 0)\n",
    "\n",
    "with open(\"stacksample/tags-1M.csv\") as tag_input:\n",
    "    tag_data = pd.read_csv(tag_input)\n",
    "    tagIndex = 0\n",
    "    for index, row in tqdm(tag_data.iterrows()):\n",
    "        currId = int(row[0])\n",
    "        currTag = row[1]\n",
    "        if currTag not in tagToTagIndex:\n",
    "            tagToTagIndex[currTag] = tagIndex\n",
    "            tagIndexToTag[tagIndex] = currTag\n",
    "            currTagIndex = tagIndex\n",
    "            tagIndex += 1\n",
    "\n",
    "        else:\n",
    "            currTagIndex = tagToTagIndex[currTag]  \n",
    "        \n",
    "        tagToFrequency[currTagIndex] += 1        \n",
    "    \n",
    "        if currId not in idToTagIndex.keys():\n",
    "            idToTagIndex[currId] = [tagToTagIndex[row[1]]]\n",
    "        else:\n",
    "            idToTagIndex[currId].append(tagToTagIndex[row[1]])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples:  53203\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of examples: \", len(idToTagIndex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c# (14): 6722 times\n",
      "java (89): 3858 times\n",
      ".net (15): 3598 times\n",
      "php (76): 3223 times\n",
      "asp.net (8): 3041 times\n",
      "javascript (132): 2852 times\n",
      "c++ (18): 2509 times\n",
      "jquery (370): 2198 times\n",
      "iphone (607): 2111 times\n",
      "python (196): 2070 times\n"
     ]
    }
   ],
   "source": [
    "# find 10 most common tags\n",
    "\n",
    "tagToFrequencyList = []\n",
    "\n",
    "for key, value in tagToFrequency.items():\n",
    "    temp = [key, value]\n",
    "    tagToFrequencyList.append(temp)\n",
    "    \n",
    "tagToFrequencyList.sort(reverse=True, key=lambda x: x[1])\n",
    "\n",
    "for tag in tagToFrequencyList[:10]:\n",
    "    print(f\"{tagIndexToTag[tag[0]]} ({tag[0]}): {tag[1]} times\")\n",
    "    \n",
    "mostCommonTags = {}\n",
    "for counter, tag in enumerate(tagToFrequencyList[:10]):   #currently takes top 10 tags\n",
    "    mostCommonTags[tag[0]] = counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{14: 0, 89: 1, 15: 2, 76: 3, 8: 4, 132: 5, 18: 6, 370: 7, 607: 8, 196: 9}\n"
     ]
    }
   ],
   "source": [
    "# list(idToTagIndex.values())[:10]\n",
    "print(mostCommonTags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Id                                               Body  \\\n",
      "0           80  I've written a database generation script in S...   \n",
      "1           90  Are there any really good tutorials explaining...   \n",
      "2          120  Has anyone got experience creating SQL-based A...   \n",
      "3          180  This is something I've pseudo-solved many time...   \n",
      "4          260  I have a little game written in C#. It uses a ...   \n",
      "...        ...                                                ...   \n",
      "53198  2495810  I'm going to host an app on a shared host and ...   \n",
      "53199  2495870  I use MouseMove event to move objects(say labe...   \n",
      "53200  2495890  I have dragged a empty asp.net table onto my w...   \n",
      "53201  2495910  I want to log some seemingly random errors I'm...   \n",
      "53202  2496040  I have a php array that has a bunch of data th...   \n",
      "\n",
      "                                                    Code Top-Tags  \n",
      "0      Create Table tRole (\\n      roleID integer Pri...       []  \n",
      "1                                                              []  \n",
      "2                                                             [4]  \n",
      "3                                                              []  \n",
      "4      ICard Cards[current] = new MyGame.CardLibrary....   [0, 2]  \n",
      "...                                                  ...      ...  \n",
      "53198                                                          []  \n",
      "53199  OnMouseMove(e MouseEventArgs)\\n    if e.Button...      [2]  \n",
      "53200  if (IsPostBack == false)\\n{\\n    // check if d...   [0, 4]  \n",
      "53201                                                          []  \n",
      "53202  var points = [\\n['test name', 37.331689, -122....   [3, 5]  \n",
      "\n",
      "[53203 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "idToTenTags = {}\n",
    "\n",
    "for postId, tags in idToTagIndex.items():\n",
    "    containsTopTenTags = [mostCommonTags[tag] for tag in tags if tag in mostCommonTags.keys()]\n",
    "    idToTenTags[postId] = containsTopTenTags\n",
    "    \n",
    "questions_data['Top-Tags'] = questions_data['Id'].apply(lambda x: idToTenTags[x])\n",
    "\n",
    "print(questions_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'the', 'to', 'i', 'a', 'is', 'and', 'in', 'of']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(questions_data['Body'].values.tolist())\n",
    "vectorizer.adapt(text_ds)\n",
    "vectorizer.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 1 0 0 0 0 0 0 0]]\n",
      "47882\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "questions_X = vectorizer(np.array([[s] for s in questions_data['Body'].values])).numpy()\n",
    "questions_y = mlb.fit_transform(np.array(questions_data['Top-Tags'].values))\n",
    "print(questions_y[:5])\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(questions_X, questions_y, train_size=0.90, random_state=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   4,  124,  176,   12,  166,    9,   40,   15,   26, 1701,    5,\n",
       "         393,  848,  291,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = vectorizer([[\"I tried running this line of code, but I'm receiving a null pointer exception\"]])\n",
    "output.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1517\n"
     ]
    }
   ],
   "source": [
    "voc = vectorizer.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))\n",
    "print(word_index[\"hello\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 166, 9, 40]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [\"this\", \"line\", \"of\", \"code\"]\n",
    "[word_index[w] for w in test] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20002\n",
      "Converted 18657 words (1343 misses)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(voc) + 2\n",
    "print(num_tokens)\n",
    "embedding_dim = 200\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    try:\n",
    "        embedding_vector = word_vect.get_vector(word)\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    except:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 200)         4000400   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_13 (Spatia (None, None, 200)         0         \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 100)               120400    \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 4,121,810\n",
      "Trainable params: 121,410\n",
      "Non-trainable params: 4,000,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, Sequential\n",
    "\n",
    "int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded_sequences = embedding_layer(int_sequences_input)\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_tokens, embedding_dim))\n",
    "model.add(layers.LSTM(128, return_sequences=True))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.LSTM(64))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(len(mostCommonTags), activation='sigmoid'))\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(embedding_layer)\n",
    "# model.add(layers.SpatialDropout1D(0.2))\n",
    "# model.add(layers.LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "# model.add(layers.Dense(len(mostCommonTags), activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 218s 580ms/step - loss: 6.2214e-05 - top_k_categorical_accuracy: 0.4550 - val_loss: 0.2313 - val_top_k_categorical_accuracy: 0.1853\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17e12c3c8>"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import tensorflow.keras.callbacks as cbks\n",
    "class CustomMetrics(cbks.Callback):\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        for k in logs:\n",
    "            print(logs[k])\n",
    "\n",
    "def custom_metric(y_true, y_pred):\n",
    "    print(\"y_true\")\n",
    "    print(K.print_tensor(y_true))\n",
    "    print(\"y_pred\")\n",
    "    print(K.print_tensor(y_pred))\n",
    "    \n",
    "  \n",
    "    \n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              #loss = custom_loss,\n",
    "              metrics = [tf.keras.metrics.TopKCategoricalAccuracy(k=3)]) \n",
    "              #metrics = 'accuracy')\n",
    "              #metrics = custom_metric)                \n",
    "\n",
    "weights = {}\n",
    "\n",
    "for i in range(10):\n",
    "    weights[i] = (1 / tagToFrequencyList[i][1])\n",
    "\n",
    "model.fit(train_X, train_y, batch_size=128, epochs=1, validation_data=(test_X, test_y), class_weight=weights, callbacks = [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average quality numbers for threshold: 0.01\n",
      "Precision: 0.0609, Recall: 1.0000, F1-measure: 0.1149\n",
      "Micro-average quality numbers for threshold: 0.02\n",
      "Precision: 0.0609, Recall: 1.0000, F1-measure: 0.1149\n",
      "Micro-average quality numbers for threshold: 0.03\n",
      "Precision: 0.0610, Recall: 0.9997, F1-measure: 0.1149\n",
      "Micro-average quality numbers for threshold: 0.04\n",
      "Precision: 0.0610, Recall: 0.9969, F1-measure: 0.1149\n",
      "Micro-average quality numbers for threshold: 0.05\n",
      "Precision: 0.0610, Recall: 0.9920, F1-measure: 0.1149\n",
      "Micro-average quality numbers for threshold: 0.06\n",
      "Precision: 0.0612, Recall: 0.9880, F1-measure: 0.1152\n",
      "Micro-average quality numbers for threshold: 0.07\n",
      "Precision: 0.0617, Recall: 0.7936, F1-measure: 0.1145\n",
      "Micro-average quality numbers for threshold: 0.08\n",
      "Precision: 0.0648, Recall: 0.4198, F1-measure: 0.1123\n",
      "Micro-average quality numbers for threshold: 0.09\n",
      "Precision: 0.0713, Recall: 0.0123, F1-measure: 0.0210\n",
      "Micro-average quality numbers for threshold: 0.1\n",
      "Precision: 0.0528, Recall: 0.0040, F1-measure: 0.0075\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "\n",
    "# string_input = keras.Input(shape=(200,), dtype=\"string\")\n",
    "predictions=model.predict([test_X])\n",
    "# print(\"test_x\")\n",
    "# print(test_X)\n",
    "#thresholds=[0.01,0.02, 0.03, 0.04, 0.05, 0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "thresholds = [0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.1]\n",
    "for val in thresholds:\n",
    "    pred=predictions.copy()\n",
    "#     print(\"pred before\")\n",
    "#     print(pred)\n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0  \n",
    "#     print(\"val: {}\", val)\n",
    "#     print(test_y.shape)\n",
    "#     print(pred.shape)\n",
    "#     print(test_X.shape)\n",
    "#     print(\"----\")\n",
    "#     print(\"test_y\")\n",
    "#     print(test_y)\n",
    "#     print(\"pred after\")\n",
    "#     print(pred)\n",
    "    precision = precision_score(test_y, pred, average='micro')\n",
    "    recall = recall_score(test_y, pred, average='micro')\n",
    "    f1 = f1_score(test_y, pred, average='micro')\n",
    "   \n",
    "    print(\"Micro-average quality numbers for threshold: \" + str(val))\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c#                0.083           \n",
      "java              0.072           \n",
      ".net              0.062           \n",
      "php               0.071           \n",
      "asp.net           0.066           \n",
      "javascript        0.084           \n",
      "c++               0.074           \n",
      "jquery            0.082           \n",
      "iphone            0.072           \n",
      "python            0.084           \n",
      "\n",
      "Most likely tag: None\n"
     ]
    }
   ],
   "source": [
    "string_input = keras.Input(shape=(1,), dtype=\"string\")\n",
    "x = vectorizer(string_input)\n",
    "preds = model(x)\n",
    "end_to_end_model = keras.Model(string_input, preds)\n",
    "\n",
    "probabilities = end_to_end_model.predict(\n",
    "#    [\"I wanted to open my app without safari system alert but I found out that is impossible. so i decided to handle this alert event but I couldn't find the way. if I click [open], then safari open App, but if I click [cancel], then 'appCheckTimer' will be executed, then safari moves to 'some page's url'. if there is no way to not open this alert, I want to handle this alert's button event, when user click [cancel], I just want to stay that page. that alert is not opened by me, it's by safari So I can't handle it.\"]\n",
    "#    [\"Dropped my iphone again :(\"]\n",
    "    [\"my name is Varuni\"]\n",
    ")\n",
    "\n",
    "for i, prob in np.ndenumerate(probabilities):\n",
    "    print('{:<16}  {:<16}'.format(tagIndexToTag[tagToFrequencyList[i[1]][0]], truncate(prob, 3)))\n",
    "prob = tagIndexToTag[tagToFrequencyList[np.argmax(probabilities)][0]]\n",
    "val = prob if np.max(probabilities) >= 0.5 else None\n",
    "print(f\"\\nMost likely tag: {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
